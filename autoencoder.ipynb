{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#math tools\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import resample\n",
    "from scipy.signal import decimate\n",
    "#machine learning\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#utils\n",
    "from time import time\n",
    "from os.path import join\n",
    "from os import listdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time()\n",
    "    s = now - since\n",
    "    m = np.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Patient Signal(2).zip', 'Healthy Signal(2)', 'HealthyMeander', 'HealthySpiral.zip', 'HelthyCircle.zip', 'PatientCircle.zip', 'PatientMeander.zip', 'PatientSpiral.zip', 'description.html', 'description_fichiers']\n"
     ]
    }
   ],
   "source": [
    "data_path=join(\"..\",\"NewHandPD\")\n",
    "folder_path=listdir(data_path)\n",
    "folder_path.sort()\n",
    "print(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_dataset(folder_path):\n",
    "    for folder in folder_path:\n",
    "        dataset=[]\n",
    "        task_path=listdir(join(data_path,folder))\n",
    "        task_path.sort()\n",
    "        for task in task_path:\n",
    "            measures=[]\n",
    "            path=join(data_path,folder,task)\n",
    "            \n",
    "            \"\"\"with open(path) as file:\n",
    "                for line in file.readlines():\n",
    "                    measures.append(line.split(\",\"))\"\"\"\n",
    "            dataset.append(np.loadtxt(path,delimiter=\",\"))#measures\n",
    "        yield np.concatenate((dataset[0],dataset[1]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "905\n"
     ]
    }
   ],
   "source": [
    "raw=next(yield_dataset(folder_path))\n",
    "\n",
    "print(len(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        #Vanilla LSTM\n",
    "        input_size: The number of expected features in the input `x`\n",
    "        hidden_size: The number of features in the hidden state `h`\n",
    "        num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n",
    "            would mean stacking two LSTMs together to form a `stacked LSTM`,\n",
    "            with the second LSTM taking in outputs of the first LSTM and\n",
    "            computing the final results. Default: 1\n",
    "        bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n",
    "            Default: ``True``\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False``\n",
    "        dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n",
    "            LSTM layer except the last layer, with dropout probability equal to\n",
    "            :attr:`dropout`. Default: 0\n",
    "        bidirectional: If ``True``, becomes a bidirectional LSTM. Default: ``False``\n",
    "        \n",
    "        #our model\n",
    "        batch_size : default : 1\n",
    "        output_size : default : 1\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size,num_layers=1, bias=True,batch_first=False,\n",
    "                 dropout=0,bidirectional=False, batch_size=1, output_size=1):\n",
    "        super(Model, self).__init__()\n",
    "        #Vanilla LSTM\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias=bias\n",
    "        self.batch_first=batch_first\n",
    "        self.dropout=dropout\n",
    "        self.bidirectional=bidirectional\n",
    "        #our model\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.encoder = torch.nn.LSTM(self.input_size, self.hidden_size, self.num_layers,self.bias,self.batch_first,\n",
    "                            self.dropout,self.bidirectional)\n",
    "        \n",
    "        #define the dropout layer\n",
    "        self.dropout_layer=torch.nn.Dropout(self.dropout)\n",
    "\n",
    "        # Define the decoder layer\n",
    "        self.decoder = torch.nn.LSTM(self.hidden_size, self.output_size)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "                \n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of encoder_out: (seq_len, batch, num_directions * hidden_size)\n",
    "        # shape of self.hidden: (h_n, c_n), where hidden state h_n and cell state c_n both \n",
    "        # have shape (num_layers * num_directions, batch, hidden_size).\n",
    "        encoder_out, self.hidden = self.encoder(input)\n",
    "        #print(encoder_out.shape)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            #sums the outputs : direction left-right and direction right-left\n",
    "            # encoder_out shape should now be (seq_len, batch,hidden_size)\n",
    "            encoder_out = encoder_out[: ,: ,: self.hidden_size] + encoder_out[: , :, self.hidden_size: ]\n",
    "        \n",
    "        # Only take the output from the final timestep\n",
    "        drop=self.dropout_layer(encoder_out[-1])\n",
    "        y_pred = self.linear(drop)\n",
    "        y_pred = self.sigmoid(y_pred)\n",
    "        return y_pred.squeeze(0)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        \"\"\"h_0 of shape (num_layers * num_directions, batch, hidden_size): \n",
    "        tensor containing the initial hidden state for each element in the batch. \n",
    "        If the RNN is bidirectional, num_directions should be 2, else it should be 1.\n",
    "\n",
    "        c_0 of shape (num_layers * num_directions, batch, hidden_size): \n",
    "        tensor containing the initial cell state for each element in the batch.\n",
    "        \n",
    "        The hidden state is modified in place.\n",
    "        \"\"\"\n",
    "        num_directions=1\n",
    "        if self.bidirectional:\n",
    "            num_directions=2\n",
    "            \n",
    "        \n",
    "        self.hidden=torch.zeros(self.num_layers*num_directions, self.batch_size, self.hidden_size),\n",
    "        torch.zeros(self.num_layers*num_directions, self.batch_size, self.hidden_size)\n",
    "    \n",
    "    def init_forget_bias(self):\n",
    "        \"\"\"Following advices of Jozefowicz et al. 2015,\n",
    "        we initialize the bias of the forget gate to a large value such as 1\n",
    "        In PyTorch, the forget gate bias is stored as b_hf in bias_hh_l[k] : \n",
    "        the learnable hidden-hidden bias of the kth layer (b_hi|b_hf|b_hg|b_ho), of shape (4*hidden_size).\n",
    "        So b_hf == bias_hh_lk[hidden_size:2*hidden_size]\n",
    "        \n",
    "        The weights are modified in-place, like init_hidden(self).\n",
    "        \"\"\"\n",
    "        gen=self.modules()\n",
    "        _=next(gen)#model summary : don't care about it\n",
    "        lstm=next(gen)\n",
    "        if not isinstance(lstm,torch.nn.LSTM):\n",
    "            raise NotImplementedError(\"the encoder should be an LSTM and should be the first module of the model\")\n",
    "        \n",
    "        with torch.no_grad():#so the optimizer doesn't know about this ;)\n",
    "            lstm.bias_hh_l0[hidden_size:2*hidden_size]=torch.ones(lstm.hidden_size)\n",
    "            if lstm.bidirectional:\n",
    "                lstm.bias_hh_l0_reverse[hidden_size:2*hidden_size]=torch.ones(lstm.hidden_size)\n",
    "            if lstm.num_layers > 1:\n",
    "                lstm.bias_hh_l1[hidden_size:2*hidden_size]=torch.ones(lstm.hidden_size)\n",
    "                if lstm.bidirectional:\n",
    "                    lstm.bias_hh_l1_reverse[hidden_size:2*hidden_size]=torch.ones(lstm.hidden_size)\n",
    "            if lstm.num_layers > 2:\n",
    "                lstm.bias_hh_l2[hidden_size:2*hidden_size]=torch.ones(lstm.hidden_size)    \n",
    "                if lstm.bidirectional:\n",
    "                    lstm.bias_hh_l2_reverse[hidden_size:2*hidden_size]=torch.ones(lstm.hidden_size)\n",
    "            if lstm.num_layers > 3:\n",
    "                lstm.bias_hh_l3[hidden_size:2*hidden_size]=torch.ones(lstm.hidden_size)    \n",
    "                if lstm.bidirectional:\n",
    "                    lstm.bias_hh_l3_reverse[hidden_size:2*hidden_size]=torch.ones(lstm.hidden_size)\n",
    "            if lstm.num_layers > 4:\n",
    "                lstm.bias_hh_l4[hidden_size:2*hidden_size]=torch.ones(lstm.hidden_size)    \n",
    "                if lstm.bidirectional:\n",
    "                    lstm.bias_hh_l4_reverse[hidden_size:2*hidden_size]=torch.ones(lstm.hidden_size)\n",
    "                                    \n",
    "            if lstm.num_layers>5:\n",
    "                raise NotImplementedError(\"you can only have max 5 layers for now\")\n",
    "                \n",
    "    def count_params(self):\n",
    "        \"\"\"returns (total n° of parameters, n° of trainable parameters)\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return total_params, trainable_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
