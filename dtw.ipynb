{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- [Data](#Data)\n",
    "- [Model](#Model)\n",
    "- [Training](#Training)\n",
    "    - [10-fold-cross-validation-(early-stopping)](#10-fold-cross-validation-(early-stopping))\n",
    "- [Evaluation](#Evaluation)\n",
    "- [Visualization](#Visualization)\n",
    "    - [Interpretation](#Interpretation)\n",
    "- [Implemented-but-not-used](#Implemented-but-not-used)\n",
    "    - [Debug](#Debug)\n",
    "\n",
    "# Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "#math tools\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import resample\n",
    "from scipy.signal import decimate\n",
    "from fastdtw import fastdtw\n",
    "#machine learning\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#utils\n",
    "from time import time\n",
    "import warnings\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "import pickle\n",
    "\n",
    "#custom\n",
    "from utils import *\n",
    "from load_data import *\n",
    "from training import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "Cf `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot(plot_i,train,valid,test,average=True):\n",
    "    if average:\n",
    "        print(results[:80].replace(\";\",\"|\"))\n",
    "        if early_stopping:\n",
    "            title = \"average {} over 10 folds over the {} first epochs\".format(index2plot[plot_i],shortest_fold)\n",
    "        else:\n",
    "            title = \"average {} over 10 folds over {} epochs\".format(index2plot[plot_i],n_epochs)\n",
    "    else :\n",
    "        train,valid,test=np.asarray(train),np.asarray(valid),np.asarray(test)\n",
    "        title=str((task_i,is_lstm,learning_rate,hidden_size,num_layers,bidirectional,\n",
    "    dropout,clip,window_size))\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.plot(train[:,plot_i],label=\"training\")\n",
    "    plt.plot(valid[:,plot_i],label=\"validation\")\n",
    "    plt.plot(test[:,plot_i],label=\"test\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(index2plot[plot_i])\n",
    "    plt.legend()\n",
    "\n",
    "def return_results(train_metrics,valid_metrics,test_metrics,early_stopping,flat_falses):\n",
    "    train_metrics,valid_metrics,test_metrics=np.asarray(train_metrics),np.asarray(valid_metrics),np.asarray(test_metrics)\n",
    "    model_name=\"LSTM\" if is_lstm else \"GRU\"\n",
    "    task_name=index2task[task_i] if task_i is not None else str(task_i)\n",
    "    results=\"{} ; {} ; {} ; {}   ; {} ; {} ; {} ; {} ; {:.2f} (± {:.2f}) ; None ; {} ; TRUE ; TRUE ; {} ; TRUE ; {:.2f} (± {:.2f}) ; {:.2f} (± {:.2f}) \".format(\n",
    "    task_name,model_name,learning_rate, hidden_size,num_layers,bidirectional,dropout,clip,\n",
    "    np.mean(early_stopping),np.std(early_stopping),compute_movement,downsampling_factor,\n",
    "     np.mean(train_metrics[:,1]),np.std(train_metrics[:,1]), np.mean(valid_metrics[:,1]),np.std(valid_metrics[:,1]))\n",
    "\n",
    "    test_metrics=test_metrics.T\n",
    "    for metric in test_metrics[1:]:#don't care about the loss\n",
    "        mean,std=np.mean(metric),np.std(metric)\n",
    "        results+=\"; {:.2f} (± {:.2f}) \".format(mean,std)\n",
    "    results+=\" ; \"\n",
    "    results+=\" ; \".join(map(str, flat_falses))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Cf `load_data.py`\n",
    "## Loading\n",
    "\n",
    "## Task selection\n",
    "set `task_i` to None if you want to train the model on all tasks at once (i.e. early fusion)  \n",
    "Else set `task_i` to the desired task index (cf. task2index)\n",
    "\n",
    "## Compute movement\n",
    "Transforms data as Zhang et al. (cf Report #5)\n",
    "\n",
    "## Scale then downsample (or not) then concatenate task id (or not)\n",
    "Set `downsampling_factor` to `1` if you don't want to downsample\n",
    "## Split in subsequence (or not)\n",
    "Set `window_size` to `None` if you don't want to split data into subsequence of fixed length  \n",
    "Set `paper_air_split` to `False` if you don't want to split data into strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading and massaging data, this might take a few seconds...\n",
      "(75-3 subjects, 8 tasks, X timesteps, 7 measures)\n",
      "72 8 1772 7\n",
      "\n",
      "task index, name\n",
      "0 spiral\n",
      "len(data), len(targets), len(data[0]) :\n",
      "72 72 1772\n",
      "\n",
      "movement was not computed (i.e. data was not transformed)\n",
      "\n",
      "scaling \n",
      "len(data), len(targets), len(data[0]) :\n",
      "72 72 1772\n",
      "the task is represented as one single sequence  (i.e. data was not transformed)\n"
     ]
    }
   ],
   "source": [
    "## Loading\n",
    "#Cf `load_data.py`\n",
    "task_i=task2index[\"spiral\"]\n",
    "compute_movement=False\n",
    "downsampling_factor=1\n",
    "window_size=None\n",
    "paper_air_split=False\n",
    "try:\n",
    "    assert window_size is None or not paper_air_split\n",
    "except AssertionError:\n",
    "    print(\"you have to choose between subsequences of fixed length and strokes !\")\n",
    "else:\n",
    "    print(\"\\nloading and massaging data, this might take a few seconds...\")\n",
    "    data_gen=load_data()\n",
    "    data,targets=[],[]\n",
    "    for subject,label in data_gen:\n",
    "        data.append(subject)\n",
    "        targets.append(label)\n",
    "    print(\"(75-3 subjects, 8 tasks, X timesteps, 7 measures)\")\n",
    "    print(len(data),len(data[0]),len(data[0][0]),len(data[0][0][0]))\n",
    "    data, targets= massage_data(data, targets,task_i, compute_movement, downsampling_factor, window_size,paper_air_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=3117\n",
    "for i,task in enumerate(data):\n",
    "    \n",
    "    if len(task) > max_len:\n",
    "        data[i]=task[:max_len]\n",
    "    else:\n",
    "        data[i]=np.concatenate((task,np.zeros(shape=(max_len-len(task),7))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 2\n",
    "\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=fastdtw, repeats=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'float' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-08193e27ebcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0massigned_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkclusterer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massign_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/nltk/cluster/util.py\u001b[0m in \u001b[0;36mcluster\u001b[0;34m(self, vectors, assign_clusters, trace)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# call abstract method to cluster the vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_vectorspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# assign the vectors to clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/nltk/cluster/kmeans.py\u001b[0m in \u001b[0;36mcluster_vectorspace\u001b[0;34m(self, vectors, trace)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_means\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_means\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_means\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cluster_vectorspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mmeanss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_means\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/nltk/cluster/kmeans.py\u001b[0m in \u001b[0;36m_cluster_vectorspace\u001b[0;34m(self, vectors, trace)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# measure the degree of change from the previous step for convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mdifference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sum_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_means\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdifference\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_difference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0mconverged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/nltk/cluster/kmeans.py\u001b[0m in \u001b[0;36m_sum_distances\u001b[0;34m(self, vectors1, vectors2)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mdifference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mdifference\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdifference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'float' and 'tuple'"
     ]
    }
   ],
   "source": [
    "assigned_clusters = kclusterer.cluster(data, assign_clusters=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
